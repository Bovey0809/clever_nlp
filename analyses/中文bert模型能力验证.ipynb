{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833d99de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "checkpoint = \"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05189d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.base_model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5554d",
   "metadata": {},
   "source": [
    "计算按照单词的tokens来分词的embeddings的相似度，以及通过模型前向之后的embeddings的相似度\n",
    "\n",
    "```python\n",
    "def take_ids_from_bert_input(tokens, positions)->embeddings\n",
    "def take_ids_from_bert_output(tokens, positions)->embeddings\n",
    "def cosine_similarity(embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5482a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.643 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['北京', '天安门']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut_sentence_into_words(sentence)->list:\n",
    "    \"\"\"对句子分词\n",
    "    \n",
    "    注意：英文也会作为一个词组被分出来。\n",
    "    \"\"\"\n",
    "    words = jieba.cut(sentence)\n",
    "    words = filter(lambda x: len(x) > 1, words)\n",
    "    return list(set(words))\n",
    "# test\n",
    "cut_sentence_into_words(\"我爱北京北京北京天安门\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf852d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 15, 16]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_position_in_tokens(tokens, word):\n",
    "    length = len(word)\n",
    "    positions = []\n",
    "    for character in word:\n",
    "        for position, token in enumerate(tokens):\n",
    "            if character == token:\n",
    "                positions.append(position)\n",
    "                break\n",
    "    assert len(word) == len(positions), f\"length of {word} in {tokens} not equal to {positions}\"\n",
    "    return positions\n",
    "# test\n",
    "get_words_position_in_tokens(\"I love 我北京北京北京天安门北京。\", \"天安门\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0458e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(word):\n",
    "    for ch in word:\n",
    "        if not (u'\\u4e00' <= ch <= u'\\u9fff'):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7f6da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('北京', [1266, 776], [7, 8]), ('天安门', [1921, 2128, 7305], [11, 12, 13])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens_positions(sentence)->tuple:\n",
    "    \"\"\"提取中文词汇在sentence转变成tokens后的position位置。\n",
    "\n",
    "    输入：\n",
    "       sentence（str） ：我爱北京天安门。\n",
    "    输出：\n",
    "        （word, word_ids, word_positions）\n",
    "    \"\"\"\n",
    "    words = cut_sentence_into_words(sentence)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sentence))\n",
    "\n",
    "    for word in words:\n",
    "        if is_chinese(word):\n",
    "            word_ids = tokenizer.convert_tokens_to_ids([*word])\n",
    "            word_positions = get_words_position_in_tokens(tokens, word)\n",
    "            yield word, word_ids, word_positions\n",
    "# test\n",
    "list(get_tokens_positions('I love china, 我爱北京北京天安门。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e402f4d",
   "metadata": {},
   "source": [
    "由 get_tokens_positons 得到原始 embeddings 中的单词的的位置，以及在 tokens 中的位置，分别用于 bert 的 word embeddings 和输出的 logits 之间的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a7254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(a, b):\n",
    "    return cosine_distances(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6ac498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8dfac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京 \n",
      "\n",
      "tensor([[1.0000, 0.3443],\n",
      "        [0.3443, 1.0000]], grad_fn=<MmBackward0>)\n",
      "tensor([[1.0000, 0.8295],\n",
      "        [0.8295, 1.0000]], grad_fn=<MmBackward0>)\n",
      "天安门 \n",
      "\n",
      "tensor([[1.0000, 0.0905, 0.0704],\n",
      "        [0.0905, 1.0000, 0.0806],\n",
      "        [0.0704, 0.0806, 1.0000]], grad_fn=<MmBackward0>)\n",
      "tensor([[1.0000, 0.7364, 0.6571],\n",
      "        [0.7364, 1.0000, 0.7519],\n",
      "        [0.6571, 0.7519, 1.0000]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"我爱北京天安门。\"\n",
    "outpus = model(**tokenizer(sentence, return_tensors='pt'))[0][0]\n",
    "for word, word_ids, token_positions in get_tokens_positions(sentence):\n",
    "    print(word, \"\\n\")\n",
    "    input_embeddings = word_embeddings[word_ids]\n",
    "    output_embeddings = outpus[token_positions]\n",
    "    print(sim_matrix(input_embeddings, input_embeddings))\n",
    "    print(sim_matrix(output_embeddings, output_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ddfe6",
   "metadata": {},
   "source": [
    "结论： 单单对于天安门，北京这两个词，训练之后的语意是相近的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f065c",
   "metadata": {},
   "source": [
    "TODO list\n",
    "- [ ] 对中文切词的时候，可能存在UNK token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e559e85",
   "metadata": {},
   "source": [
    "对于中文新华词典数据集里面的所有definition，计算一下 cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee0d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e595c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-405a8ea61a95b513\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-405a8ea61a95b513/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1016f4c4bb46b3a89006766dbce40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"csv\", data_files=[\"../data/xinhua2.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0ffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11573f",
   "metadata": {},
   "source": [
    "- [x] 对数据集塞选，保证 definition 里面的词必须出现在 tokenizer.vacab 里面\n",
    "- [x] 对数据集塞选，保证 word 里面的词必须出现在 tokenizer.vacab 里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03770006",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab\n",
    "def filter_function(x):\n",
    "    for word in x.values():\n",
    "        for ch in word:\n",
    "            if ch not in vocab:\n",
    "                return False\n",
    "    return True\n",
    "# test\n",
    "assert filter_function({\"word\": \"我们\", \"definition\": \"我们几个.\"}) \n",
    "assert filter_function({\"word\": \"我们\", \"definition\": \"我们衽个.\"}) is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd9c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-405a8ea61a95b513/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-eaf1b622cd38c32f.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(filter_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fa1f1",
   "metadata": {},
   "source": [
    "保证完之后，只剩下 284976 个词条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3848effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帝王的诏书﹑制令。\n",
      "诏书\n",
      "tensor([[1.0000, 0.1497],\n",
      "        [0.1497, 1.0000]], grad_fn=<MmBackward0>)\n",
      "tensor([[1.0000, 0.7741],\n",
      "        [0.7741, 1.0000]], grad_fn=<MmBackward0>)\n",
      "帝王\n",
      "tensor([[1.0000, 0.3293],\n",
      "        [0.3293, 1.0000]], grad_fn=<MmBackward0>)\n",
      "tensor([[1.0000, 0.8300],\n",
      "        [0.8300, 1.0000]], grad_fn=<MmBackward0>)\n",
      "制令\n",
      "tensor([[1.0000, 0.1327],\n",
      "        [0.1327, 1.0000]], grad_fn=<MmBackward0>)\n",
      "tensor([[1.0000, 0.6989],\n",
      "        [0.6989, 1.0000]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for sentence in dataset['train']['definition']:\n",
    "    print(sentence)\n",
    "    outpus = model(**tokenizer(sentence, return_tensors='pt'))[0][0]\n",
    "    for word, word_ids, token_positions in get_tokens_positions(sentence):\n",
    "        print(word)\n",
    "        input_embeddings = word_embeddings[word_ids]\n",
    "        output_embeddings = outpus[token_positions]\n",
    "        input_sim = sim_matrix(input_embeddings, input_embeddings)\n",
    "        print(input_sim)\n",
    "        output_sim = sim_matrix(output_embeddings, output_embeddings)\n",
    "        print(output_sim)\n",
    "        # if ((output_sim+0.00001) < input_sim).sum() != 0:\n",
    "            # print(word)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8299f",
   "metadata": {},
   "source": [
    "还需要验证一点：是不是只要是经过 bert 的 tokens 的距离都会拉近。\n",
    "\n",
    "设计实验：\n",
    "1. 判断是不是只要经过bert，向量的距离就会变近\n",
    "2. 输入：任意definition，任意token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03c0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '中', '国', '天', '安', '门', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "test_case = \"中国天安门。\"\n",
    "inputs = tokenizer(test_case, return_tensors='pt')\n",
    "print(tokenizer.convert_ids_to_tokens(*inputs['input_ids']))\n",
    "output_embed = model(**inputs)[0][0]\n",
    "\n",
    "input_ids = tokenizer.encode(test_case)\n",
    "\n",
    "input_embed = word_embeddings[input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb2bd77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000001 ,  0.01460356,  0.09307431,  0.01304194, -0.05687577,\n",
       "        -0.02262287,  0.34448862,  0.5507547 ],\n",
       "       [ 0.01460356,  0.99999964,  0.16015863,  0.12534174,  0.06934987,\n",
       "         0.03584412,  0.16264743,  0.07302652],\n",
       "       [ 0.09307431,  0.16015863,  1.0000008 ,  0.09747883,  0.05757412,\n",
       "         0.1834615 ,  0.12305939,  0.10989287],\n",
       "       [ 0.01304194,  0.12534174,  0.09747883,  0.9999992 ,  0.09052134,\n",
       "         0.07040579,  0.08258034,  0.04834021],\n",
       "       [-0.05687577,  0.06934987,  0.05757412,  0.09052134,  0.9999995 ,\n",
       "         0.08060817,  0.02444113, -0.00397609],\n",
       "       [-0.02262287,  0.03584412,  0.1834615 ,  0.07040579,  0.08060817,\n",
       "         1.0000001 ,  0.01778734,  0.02921772],\n",
       "       [ 0.34448862,  0.16264743,  0.12305939,  0.08258034,  0.02444113,\n",
       "         0.01778734,  0.99999976,  0.36590734],\n",
       "       [ 0.5507547 ,  0.07302652,  0.10989287,  0.04834021, -0.00397609,\n",
       "         0.02921772,  0.36590734,  1.0000006 ]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(input_embed, input_embed).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ceb26a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999976, 0.47359958, 0.46130764, 0.4001449 , 0.36885065,\n",
       "        0.5084616 , 0.6717285 , 0.62997663],\n",
       "       [0.47359958, 1.0000005 , 0.72016007, 0.57480735, 0.57306653,\n",
       "        0.55845803, 0.52736896, 0.39728093],\n",
       "       [0.46130764, 0.72016007, 1.0000002 , 0.5515194 , 0.6074523 ,\n",
       "        0.58084834, 0.56016934, 0.39864472],\n",
       "       [0.4001449 , 0.57480735, 0.5515194 , 1.0000005 , 0.7175662 ,\n",
       "        0.63551086, 0.51203763, 0.35867494],\n",
       "       [0.36885065, 0.57306653, 0.6074523 , 0.7175662 , 0.9999999 ,\n",
       "        0.7296189 , 0.4712883 , 0.37413248],\n",
       "       [0.5084616 , 0.55845803, 0.58084834, 0.63551086, 0.7296189 ,\n",
       "        0.9999994 , 0.6280204 , 0.47663143],\n",
       "       [0.6717285 , 0.52736896, 0.56016934, 0.51203763, 0.4712883 ,\n",
       "        0.6280204 , 0.9999999 , 0.51364434],\n",
       "       [0.62997663, 0.39728093, 0.39864472, 0.35867494, 0.37413248,\n",
       "        0.47663143, 0.51364434, 1.0000005 ]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(output_embed, output_embed).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818979a",
   "metadata": {},
   "source": [
    "从这个例子看, 经过 bert token 之间的距离都变近了，这个是合理的，距离最近的是 中， 国两个token。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
