{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0db1991",
   "metadata": {},
   "source": [
    "This notebook is mainly used for demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76720639",
   "metadata": {},
   "source": [
    "### TODO list\n",
    "- [x] 思考怎么展示demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f3f07",
   "metadata": {},
   "source": [
    "- [x] 取出模长较小和模长较大的单词, 证明模长较小的是低频词汇, 模长较大的是高频词汇.\n",
    "- [x] 对于低频词汇, 在 embeddings 中找到模长, 并计算和这个单词cos角度接近的10个词汇.\n",
    "- [x] 对于低频词汇, 训练之后取出 embeddings, 计算cos角度最接近的10个词汇.\n",
    "- [ ] 通过上述过程证明, recnn 训练过后的 embeddings 语义更好."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74a537db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b52aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6cd11c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_embeddings = bert_model.embeddings.word_embeddings.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b17ac71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f5ab2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_norms = original_embeddings.norm(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270647c9",
   "metadata": {},
   "source": [
    "## 取出模长较小和模长较大的单词, 证明模长较小的是低频词汇, 模长较大的是高频词汇."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f2e9b4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n",
      "##omba\n",
      "##rdon\n",
      "[CLS]\n",
      "##anor\n",
      "##lho\n",
      "840\n",
      "##lland\n",
      "930\n",
      "690\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "# 模长最大的K个\n",
    "for token in tokenizer.convert_ids_to_tokens(original_norms.argsort(descending=True))[:K]:\n",
    "    print(tokenizer.convert_tokens_to_string([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a74ff789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\n",
      ".\n",
      ";\n",
      "the\n",
      ",\n",
      "of\n",
      "his\n",
      "(\n",
      "in\n",
      "her\n"
     ]
    }
   ],
   "source": [
    "# 模长最短的K个\n",
    "for token in tokenizer.convert_ids_to_tokens(original_norms.argsort(descending=False))[:K]:\n",
    "    print(tokenizer.convert_tokens_to_string([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06c6f0",
   "metadata": {},
   "source": [
    "结论: 模长越长, token越奇怪, 模长越短, 越常见"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee605c",
   "metadata": {},
   "source": [
    "- [ ]  为了方便演示, 找出 wordnet 和 bert 共有词汇中的低频词, 用来展示效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd3721ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b0556625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "wordnet = word_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c9e20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = list(set(wordnet.keys()) & set(tokenizer.vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99553f4",
   "metadata": {},
   "source": [
    "计算common words 中的每个词汇的 embeddings, 找出低频词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb0f4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_norms = {i:original_norms[tokenizer.convert_tokens_to_ids(i)] for i in common_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c0ff684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gallon',\n",
       " 'sock',\n",
       " 'wrestle',\n",
       " 'shave',\n",
       " 'devote',\n",
       " 'plead',\n",
       " 'kettle',\n",
       " 'preach',\n",
       " 'weigh',\n",
       " 'spoil',\n",
       " 'tread',\n",
       " 'owe',\n",
       " 'bracket',\n",
       " 'scramble',\n",
       " 'courtesy',\n",
       " 'casualty',\n",
       " 'vain',\n",
       " 'appendix',\n",
       " 'chew',\n",
       " 'coma']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的10个norms的单词\n",
    "K = 20\n",
    "common_words_norms_sorted = dict(sorted(common_words_norms.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "list(common_words_norms_sorted.keys())[:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "313c5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'national',\n",
       " 'brown',\n",
       " 'film',\n",
       " 'east',\n",
       " 'friend',\n",
       " 'girl',\n",
       " 'father',\n",
       " 'album',\n",
       " 'beautiful',\n",
       " 'north',\n",
       " 'second',\n",
       " 'village',\n",
       " 'south',\n",
       " 'new',\n",
       " 'create',\n",
       " 'brother',\n",
       " 'small',\n",
       " 'large',\n",
       " 'have']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最短的K个\n",
    "list(common_words_norms_sorted.keys())[-K:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff476f6e",
   "metadata": {},
   "source": [
    "利用完整的单词, 再次证明, 模长更长的单词属于低频词."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772c13c",
   "metadata": {},
   "source": [
    "我们以模长最长的单词为例子, 找embeddings中与之cos夹角最接近的K个单词."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a3fb572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wrestle',\n",
       " 'wrestled',\n",
       " 'wrestling',\n",
       " 'wrestlers',\n",
       " '1762',\n",
       " 'জ',\n",
       " '1713',\n",
       " '1737',\n",
       " '1727',\n",
       " '1712',\n",
       " 'ذ',\n",
       " '1757',\n",
       " '1711',\n",
       " 'wrestler',\n",
       " 'タ',\n",
       " '1734',\n",
       " '1642',\n",
       " '1781',\n",
       " '1733',\n",
       " '香']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"wrestle\"\n",
    "# gallon's embeddings\n",
    "example_embedding = original_embeddings[tokenizer.convert_tokens_to_ids(example)]\n",
    "\n",
    "# calculate cosine distance\n",
    "import torch\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "cosine_similarity_of_example = cosine_sim(original_embeddings, example_embedding)\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(cosine_similarity_of_example.argsort(descending=True)[:K])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d2c4e",
   "metadata": {},
   "source": [
    "上面的词汇大部分的都是空的, 再对高频词做同样的处理, 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c32b8c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'women',\n",
       " 'girl',\n",
       " 'female',\n",
       " 'man',\n",
       " 'lady',\n",
       " 'girls',\n",
       " '##woman',\n",
       " 'person',\n",
       " 'ladies',\n",
       " 'men',\n",
       " 'feminine',\n",
       " '238',\n",
       " 'femme',\n",
       " '234',\n",
       " '259',\n",
       " 'redhead',\n",
       " 'wife',\n",
       " '236',\n",
       " '277']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"woman\"\n",
    "# gallon's embeddings\n",
    "example_embedding = original_embeddings[tokenizer.convert_tokens_to_ids(example)]\n",
    "\n",
    "# calculate cosine distance\n",
    "import torch\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "cosine_similarity_of_example = cosine_sim(original_embeddings, example_embedding)\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(cosine_similarity_of_example.argsort(descending=True)[:K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f8d597c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'women',\n",
       " 'girl',\n",
       " 'female',\n",
       " 'man',\n",
       " 'lady',\n",
       " 'girls',\n",
       " '##woman',\n",
       " 'person',\n",
       " 'ladies',\n",
       " 'men',\n",
       " 'feminine',\n",
       " '238',\n",
       " 'femme',\n",
       " '234',\n",
       " '259',\n",
       " 'redhead',\n",
       " 'wife',\n",
       " '236',\n",
       " '277']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_original_related_words(example):\n",
    "    example_embedding = original_embeddings[tokenizer.convert_tokens_to_ids(example)]\n",
    "    # calculate cosine distance\n",
    "    cosine_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "    cosine_similarity_of_example = cosine_sim(original_embeddings, example_embedding)\n",
    "\n",
    "    return tokenizer.convert_ids_to_tokens(cosine_similarity_of_example.argsort(descending=True)[:K])\n",
    "show_original_related_words('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898bf1e",
   "metadata": {},
   "source": [
    "对于低频词, embeddings的语义相关单词较少, 对于高频词, 语义更加丰富.\n",
    "\n",
    "训练recnn网络, 通过网络调整低频词embeddings的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1539b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topK_distance_by_mse(embeddings, pred_embeddings, K=10):\n",
    "    return ((embeddings - pred_embeddings)**2).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cbb3104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female child\n",
      "girl: 3.9972944259643555\n",
      "['[unused15]', '[unused528]', '[unused557]', '[unused193]', '[unused685]', '[unused692]', '[unused418]', '[unused625]', '[unused443]', '[unused122]', '[unused101]', '[unused275]', '[unused51]', '[unused364]', '[unused147]', '[unused190]', '[unused677]', '[unused114]', '[unused66]', '[unused253]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9622)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import DictNet\n",
    "\n",
    "recnn = torch.load(\"./recnn-last.pt\")\n",
    "\n",
    "example = \"girl\"\n",
    "def get_related_words_from_recnn(example):\n",
    "    try:\n",
    "        input_sentence = wordnet[example]\n",
    "    except KeyError as e:\n",
    "        return \"words should be in the wordnet dictionary.\"\n",
    "    print(input_sentence)\n",
    "    res = tokenizer(input_sentence, return_tensors='pt').to('cuda')\n",
    "\n",
    "    res['word_ids'] = torch.tensor(tokenizer.convert_tokens_to_ids(example)).to('cuda')\n",
    "    recnn.eval()\n",
    "    output = recnn(**res)\n",
    "\n",
    "    new_gallon_embed = output['pred_embed'].to('cpu')\n",
    "    print(f\"{example}: {new_gallon_embed.norm()}\")\n",
    "\n",
    "    return tokenizer.convert_ids_to_tokens(calculate_topK_distance_by_mse(original_embeddings, new_gallon_embed).argsort()[:K])\n",
    "print(get_related_words_from_recnn(example))\n",
    "\n",
    "# original nrom\n",
    "original_norms[tokenizer.convert_tokens_to_ids(example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439df4b",
   "metadata": {},
   "source": [
    "训练之后的模长变短了, 但是语义相近的词并不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db0500ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nocturnal bird\n",
      "owl: 3.9220004081726074\n",
      "['[unused528]', '[unused418]', '[unused193]', '[unused685]', '[unused253]', '[unused114]', '[unused275]', '[unused692]', '[unused557]', '[unused483]', '[unused129]', '[unused388]', '[unused122]', '[unused339]', '[unused390]', '[unused547]', '[unused189]', '[unused164]', '[unused177]', '[unused471]']\n",
      "original norm tensor(1.3728)\n",
      "\n",
      "\n",
      "['owl', 'owls', '1779', '1795', '1675', '1672', '1738', '1679', '1781', '1819', '1611', '1802', '1785', '1646', '1749', '1752', '1771', '1642', '1659', '1682']\n"
     ]
    }
   ],
   "source": [
    "example = 'owl'\n",
    "print(get_related_words_from_recnn(example))\n",
    "print(\"original norm\", original_norms[tokenizer.convert_tokens_to_ids(example)])\n",
    "print(\"\\n\")\n",
    "print(show_original_related_words(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88ba8ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able\n",
      "absolute\n",
      "abstract\n",
      "applied\n",
      "false\n",
      "functional\n",
      "high\n",
      "application\n",
      "bow\n",
      "fold\n"
     ]
    }
   ],
   "source": [
    "longest = 0\n",
    "for k, v in wordnet.items():\n",
    "    if len(v.split(' ')) > longest:\n",
    "        longest = len(v.split(\" \"))\n",
    "        print(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
