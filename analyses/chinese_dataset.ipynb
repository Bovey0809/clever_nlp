{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b340e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b393f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_dict_path = \"../data/xinhua2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57fd6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(chinese_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e378a14",
   "metadata": {},
   "source": [
    "清洗数据集\n",
    "1. 去掉无用的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e979cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1dd2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 340290 entries, 0 to 340289\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   word        340290 non-null  object\n",
      " 1   definition  340290 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "630fb357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>340290</td>\n",
       "      <td>340290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>264037</td>\n",
       "      <td>303363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>青龙</td>\n",
       "      <td>象声词。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>22</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word definition\n",
       "count   340290     340290\n",
       "unique  264037     303363\n",
       "top         青龙       象声词。\n",
       "freq        22        360"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c7af1",
   "metadata": {},
   "source": [
    "查看有没有重复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a209f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(clean_data[clean_data.duplicated()]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c1b33",
   "metadata": {},
   "source": [
    "设计dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2caa1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['length'] = clean_data.definition.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31e43956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>340290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.022037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.230879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>968.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              length\n",
       "count  340290.000000\n",
       "mean       16.022037\n",
       "std        25.230879\n",
       "min         1.000000\n",
       "25%         5.000000\n",
       "50%         8.000000\n",
       "75%        14.000000\n",
       "max       968.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb8745",
   "metadata": {},
   "source": [
    "设计 dataset 类，根据释义的长度构建数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a4815",
   "metadata": {},
   "source": [
    "判断词语的高低频"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba534803",
   "metadata": {},
   "source": [
    "判断原始的 bert 的 embeddings 对于中文的高低频。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fd73e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-bert-wwm-ext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dde18",
   "metadata": {},
   "source": [
    "高频词汇应该具有较长的模值\n",
    "高频词汇如何获取：\n",
    "1. 通过bert的vocab先看看有多少词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c13fbc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21128, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3834a48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'vocab.txt', 'tokenizer_file': 'tokenizer.json'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2efd5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '我', '爱', '北', '京', '。', '[SEP]']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"我爱北京。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf4d71",
   "metadata": {},
   "source": [
    "构建用于训练的dataset\n",
    "1. dataset 需要一个 word id 用来指示在 tokens 中的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7c5d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xinhua_dict = \"../data/xinhua2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07ca2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09be4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-405a8ea61a95b513\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-405a8ea61a95b513/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71021cd7e6a34abcb454d2c61863e29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xinhua_dataset = datasets.load_dataset('csv', data_files=[xinhua_dict]).remove_columns('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea0ff53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word', 'definition'],\n",
       "        num_rows: 340290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinhua_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faa8ea36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter dataset\n",
    "# 去掉有unknown的单词, 去掉所有不是中文的word\n",
    "vocab = tokenizer.vocab\n",
    "def filter_unk(x):\n",
    "    for word in x.values():\n",
    "        for ch in word:\n",
    "            if ch not in vocab:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# test\n",
    "filter_unk({'word':\"你好\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "568adee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-405a8ea61a95b513/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-27275ebe44a8b2a7.arrow\n"
     ]
    }
   ],
   "source": [
    "xinhua_dataset = xinhua_dataset.filter(filter_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64088dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217f1f8bc78d48ad8ce6c710ebab62a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284976 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word', 'definition', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 284976\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_function(example):\n",
    "    definition = example['definition']\n",
    "    inputs = tokenizer(definition)\n",
    "    word = example['word']\n",
    "    word_ids = tokenizer.convert_tokens_to_ids([*word])\n",
    "    inputs['word_ids'] = word_ids\n",
    "    return inputs\n",
    "\n",
    "xinhua_dataset.map(map_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
